# Tackling the widespread and critical impact of batch effects in high-throughput data

## Summary of the papers
Batch effect is recgonized as a common and powerful source of variation in high-throughput experiments, resulted from sub-groups of measurements with qualitatively different
behaviour across conditions. The authors stated that the existence of large batch effects can impact validity of the biological conclusions. They have firstly provided an example on normalization, showing how downstream analysis can be largely affected by batch effects, leading to wrong findings, as in the sTCC case. They also showed how differences in lab protocols create batch effects in studies. Though processing date or group is commonly used to address these effects, they might represent only a part of the real variations, like environmental factors. They suggested the use of principal components to detect and separate biological and technical variations. The authors hence conducted a study examining batch effects across multiple datasets, using surrogate factors to assess their correlation with study outcomes, identifying features susceptible to batch effects. These wrong conclusions can lead to substantial resource misallocation and lack of reproducibility in predictions. Better experimental designs and statistical analysis were recommended by the authors.

## Reaction to the papers
Batch effects present a critical challenge in high-throughput scientific studies, impacting the reliability of findings and potentially leading to incorrect conclusions. In the paper, the authods mentioned that in big project like the 1000 Genomes Project, they have found a large portion of features associated with the second principal component which could not be well explained by biology alone, suggesting technical influences outweighed biological variability in multiple experimental conditions. Projects like this in fact have a great impact and their results would be used by other scientists, which can wrongly lead to an accumulation of incorrect scientific findings and unnecessary costs. Here we should emphasize again the necessity to include adjustments for batch effects as a standard practice in high-throughput data analysis, though I consider many scientists nowadays are following. We also should notice the importance of incorporating these adjustments into normalization and all other processing to ensure the accuracy and reliability of scientific findings.
 

## Questions for discussion
Does effective collaboration between these two groups enhance our ability to identify, understand, and mitigate batch effects?

What are other possible effective statistical solutions to handle batch effects?
